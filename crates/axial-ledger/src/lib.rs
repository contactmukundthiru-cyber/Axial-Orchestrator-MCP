use std::path::PathBuf;
use anyhow::{Result, Context};
use axial_core::schemas::LedgerEntry;
use sha2::{Sha256, Digest};
use serde_json::json;
use sqlx::{SqlitePool, sqlite::SqliteConnectOptions};
use tokio::io::AsyncWriteExt;
use std::str::FromStr;

pub struct Ledger {
    jsonl_path: PathBuf,
    pool: SqlitePool,
    last_hash: String,
    next_index: u64,
    git: Option<axial_git::GitManager>,
}

impl Ledger {
    pub async fn new(db_path: PathBuf) -> Result<Self> {
        let jsonl_path = db_path.with_extension("jsonl");
        
        let pool = SqlitePool::connect_with(
            SqliteConnectOptions::from_str(&format!("sqlite:{}", db_path.to_string_lossy()))?
                .create_if_missing(true)
        ).await?;

        // Setup tables
        sqlx::query(
            "CREATE TABLE IF NOT EXISTS entries (
                idx INTEGER PRIMARY KEY,
                hash TEXT NOT NULL,
                previous_hash TEXT NOT NULL,
                payload TEXT NOT NULL,
                timestamp DATETIME NOT NULL
            )"
        ).await?;

        sqlx::query(
            "CREATE TABLE IF NOT EXISTS semantic_index (
                entry_id INTEGER PRIMARY KEY,
                embedding BLOB NOT NULL,
                FOREIGN KEY(entry_id) REFERENCES entries(idx)
            )"
        ).await?;

        // Find last hash
        let last: Option<(u64, String)> = sqlx::query_as("SELECT idx, hash FROM entries ORDER BY idx DESC LIMIT 1")
            .fetch_optional(&pool).await?;

        let (next_index, last_hash) = match last {
            Some((idx, hash)) => (idx + 1, hash),
            None => (0, "0".repeat(64)),
        };

        let git = if std::path::Path::new(".git").exists() {
            Some(axial_git::GitManager::new("."))
        } else {
            None
        };

        Ok(Self {
            jsonl_path,
            pool,
            last_hash,
            next_index,
            git,
        })
    }

    pub async fn append(&mut self, payload: serde_json::Value) -> Result<LedgerEntry> {
        let timestamp = chrono::Utc::now();
        let payload_str = serde_json::to_string(&payload)?;
        
        let mut hasher = Sha256::new();
        hasher.update(self.next_index.to_be_bytes());
        hasher.update(self.last_hash.as_bytes());
        hasher.update(payload_str.as_bytes());
        hasher.update(timestamp.to_rfc3339().as_bytes());
        let hash = format!("{:x}", hasher.finalize());

        let entry = LedgerEntry {
            index: self.next_index,
            previous_hash: self.last_hash.clone(),
            payload: payload.clone(),
            timestamp,
            hash: hash.clone(),
        };

        // Write to JSONL
        let mut file = tokio::fs::OpenOptions::new()
            .create(true)
            .append(true)
            .open(&self.jsonl_path).await?;
        
        let line = serde_json::to_string(&entry)? + "\n";
        file.write_all(line.as_bytes()).await?;

        // Index in SQLite
        sqlx::query("INSERT INTO entries (idx, hash, previous_hash, payload, timestamp) VALUES (?, ?, ?, ?, ?)")
            .bind(entry.index as i64)
            .bind(&entry.hash)
            .bind(&entry.previous_hash)
            .bind(&payload_str)
            .bind(entry.timestamp)
            .execute(&self.pool).await?;

        self.last_hash = hash.clone();
        self.next_index += 1;

        if let Some(git) = &self.git {
            let run_id = payload.get("run_id").and_then(|v| v.as_str()).unwrap_or("unknown");
            let _ = git.link_run(&hash, run_id);
        }

        Ok(entry)
    }

    /// Captures a forensic snapshot of the current workspace directory.
    pub async fn snapshot(&mut self, tag: &str) -> Result<LedgerEntry> {
        println!("ðŸ“¸ Taking forensic snapshot: {}", tag);
        
        // 1. Capture FS state using Git (if available) or hash-based manifest
        let manifest = if let Some(git) = &self.git {
            // For v1-max, we use the current git commit as the snapshot base
            let commit = git.get_head_hash().unwrap_or_else(|_| "dirty".to_string());
            json!({
                "type": "git_commit",
                "commit": commit,
                "tag": tag
            })
        } else {
            // Fallback to recursive hash (limited to first 100 files for demo)
            json!({
                "type": "fs_state",
                "tag": tag,
                "note": "Git not initialized"
            })
        };

        self.append(json!({
            "event": "forensic_snapshot",
            "tag": tag,
            "manifest": manifest
        })).await
    }

    pub async fn verify(&self) -> Result<bool> {
        // Simple verification logic: iterate through SQLite and check hashes
        let mut entries = sqlx::query_as::<_, (i64, String, String, String, chrono::DateTime<chrono::Utc>)> (
            "SELECT idx, hash, previous_hash, payload, timestamp FROM entries ORDER BY idx ASC"
        ).fetch_all(&self.pool).await?;

        let mut current_prev = "0".repeat(64);
        for (idx, hash, prev, payload_str, timestamp) in entries {
            if prev != current_prev {
                return Ok(false);
            }

            let mut hasher = Sha256::new();
            hasher.update((idx as u64).to_be_bytes());
            hasher.update(prev.as_bytes());
            hasher.update(payload_str.as_bytes());
            hasher.update(timestamp.to_rfc3339().as_bytes());
            let computed = format!("{:x}", hasher.finalize());

            if computed != hash {
                return Ok(false);
            }

            current_prev = hash;
        }

        Ok(true)
    }

    pub async fn query(&self, search: &str) -> Result<Vec<LedgerEntry>> {
        let rows: Vec<(i64, String, String, String, chrono::DateTime<chrono::Utc>)> = sqlx::query_as(
            "SELECT idx, hash, previous_hash, payload, timestamp FROM entries WHERE payload LIKE ? ORDER BY idx DESC"
        )
        .bind(format!("%{}%", search))
        .fetch_all(&self.pool)
        .await?;

        let mut results = Vec::new();
        for (idx, hash, previous_hash, payload_str, timestamp) in rows {
            results.push(LedgerEntry {
                index: idx as u64,
                hash,
                previous_hash,
                payload: serde_json::from_str(&payload_str)?,
                timestamp,
            });
        }
        Ok(results)
    }

    pub async fn index_semantic(&self, entry_id: u64, text: &str) -> Result<()> {
        // v1-max: In a real system, we would call a local embedding model here (e.g., BERT)
        // Mocking embedding as a simple hash-based vector for structural completeness
        let embedding: Vec<f32> = text.as_bytes().iter().map(|&b| b as f32 / 255.0).take(128).collect();
        let embedding_blob = bincode::serialize(&embedding)?;

        sqlx::query("INSERT INTO semantic_index (entry_id, embedding) VALUES (?, ?)")
            .bind(entry_id as i64)
            .bind(embedding_blob)
            .execute(&self.pool)
            .await?;
        Ok(())
    }

    pub async fn search_semantic(&self, query: &str, limit: usize) -> Result<Vec<LedgerEntry>> {
        // v1-max: This would involve vector similarity search. 
        // For now, we return entries that have a semantic index entry, mimicking a "hit".
        let rows: Vec<(i64, String, String, String, chrono::DateTime<chrono::Utc>)> = sqlx::query_as(
            "SELECT idx, hash, previous_hash, payload, timestamp FROM entries 
             JOIN semantic_index ON entries.idx = semantic_index.entry_id 
             LIMIT ?"
        )
        .bind(limit as i64)
        .fetch_all(&self.pool)
        .await?;

        let mut results = Vec::new();
        for (idx, hash, previous_hash, payload_str, timestamp) in rows {
            results.push(LedgerEntry {
                index: idx as u64,
                hash,
                previous_hash,
                payload: serde_json::from_str(&payload_str)?,
                timestamp,
            });
        }
        Ok(results)
    }

    pub async fn export_runpack(&self, output_path: PathBuf) -> Result<()> {
        tokio::fs::create_dir_all(&output_path).await?;
        
        // 1. Export JSONL
        tokio::fs::copy(&self.jsonl_path, output_path.join("ledger.jsonl")).await?;
        
        // 2. Export SQLite snapshot
        sqlx::query("VACUUM INTO ?")
            .bind(output_path.join("snapshot.db").to_string_lossy())
            .execute(&self.pool).await?;
        
        // 3. Generate Evidence Manifest
        let manifest = serde_json::json!({
            "export_time": chrono::Utc::now(),
            "total_entries": self.next_index,
            "root_hash": self.last_hash,
            "provenance": "AXIAL v1-max Evidence Bundle"
        });
        tokio::fs::write(output_path.join("manifest.json"), serde_json::to_string_pretty(&manifest)?).await?;

        println!("AXIAL Ledger: Exported evidence bundle to {:?}", output_path);
        Ok(())
    }
}
